{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Description\n",
    "\n",
    "This notebook processes audio datasets by extracting embeddings using the **Birdnetlib library** and saving the results in Parquet format (with optional NPZ support). It is designed to handle multiple bird species datasets and ensures efficient processing with memory management, skip logic for already-processed files, and progress tracking\n",
    "\n",
    "### **Main Tasks:**\n",
    "\n",
    "1. **Load and Organize Audio Files:**\n",
    "   - Dynamically retrieves `.wav` audio files from the Extended_audios directory for the following datasets:\n",
    "     - `chiffchaff-fg`\n",
    "     - `littleowl-fg`\n",
    "     - `pipit-fg`\n",
    "     - `rtbc-begging`\n",
    "     - `littlepenguin-display_call-exhale`\n",
    "     - `greatTit_song-files`\n",
    "   - Prints the number of audio files found for each dataset and verifies the first few file paths.\n",
    "\n",
    "2. **Set Up Output Directories:**\n",
    "   - Creates an output directory (Embeddings_from_3sPadding) if it does not exist.\n",
    "   - **Parquet output**:\n",
    "      - Creates a dedicated subfolder per dataset containing multiple small `part_*.parquet` files for incremental appends.\n",
    "      - Maintains a **registry** file in Parquet format to track processed audio files and skip them on subsequent runs.\n",
    "   - **NPZ output** (optional):\n",
    "      - Creates sharded `.npz` files (e.g., `_shard000.npz`) with embeddings, plus a registry file.\n",
    "\n",
    "3. **Extract Embeddings Using BirdNET:**\n",
    "   - Initializes the BirdNET model (`Analyzer`).\n",
    "   - Iterates through the audio files, skipping already processed files (tracked in the CSV).\n",
    "   - For each audio file:\n",
    "     - Loads the file into BirdNET.\n",
    "     - Extracts embeddings and metadata (e.g., start time, end time).\n",
    "     - Saves results incrementally:\n",
    "         - **Parquet** → Appends to `part_*.parquet` files in batches of 1000.\n",
    "         - **NPZ** → Appends to in-memory shard until reaching `NPZ_SHARD_SIZE`, then writes to disk.\n",
    "\n",
    "4. **Handle Errors and Memory Management:**\n",
    "   - Skips files that cannot be processed and logs errors.\n",
    "   - Clears memory after processing each batch to ensure efficient execution.\n",
    "\n",
    "5. **Post-Processing Analysis:**\n",
    "   - Compares the original list of audio files with the processed files in the CSV.\n",
    "   - Reports the total number of processed, unprocessed, and skipped files.\n",
    "   - Displays a list of unprocessed audio files for further investigation.\n",
    "\n",
    "### **Output:**\n",
    "-  **Parquet parts** for each dataset, saved under:\n",
    "  ```\n",
    "  Output_files/Embeddings_from_3sPadding/<dataset_name>_parquet_parts/\n",
    "  ```\n",
    "  Example:  \n",
    "  `Output_files/Embeddings_from_3sPadding/littleowl_parquet_parts/part_0000.parquet`\n",
    "  `Output_files/Embeddings_from_3sPadding/littleowl_parquet_parts/littleowl_processed_files.parquet`\n",
    "\n",
    "- *(Optional)* **NPZ shards** for each dataset, saved under:\n",
    "   `Output_files/Embeddings_from_3sPadding/<dataset_name>_npz_shards/`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Features:**\n",
    "- **Efficient incremental storage** using Parquet (fast, compressed, and columnar) or NPZ (compact for ML training).\n",
    "- **Skip logic** to avoid reprocessing already-processed audio files.\n",
    "- **Batch/shard writing** to improve speed and reduce memory load.\n",
    "- **Dataset registry** to track progress across runs.\n",
    "- **Detailed stats** on processed vs. pending files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c734b515372412ba1296c9e1818497d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select dataset:', layout=Layout(width='50%'), options=(('Select a dataset', None), ('chi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected: KiwiTrimmed\n",
      "Total audio files: 455\n",
      "batch size: 5000 audio files per batch\n",
      "Output: Parquet parts directory -> kiwi_parquet_parts\n",
      "Processed registry: kiwi_processed_files.parquet\n"
     ]
    }
   ],
   "source": [
    "# === Core imports ===\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "from birdnetlib import Recording\n",
    "from birdnetlib.analyzer import Analyzer\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Output configuration\n",
    "# =========================\n",
    "OUTPUT_FORMAT = \"parquet\"   # or \"npz\"\n",
    "PARQUET_COMPRESSION = \"zstd\"   # \"snappy\" also fine\n",
    "PARQUET_ENGINE = \"pyarrow\"\n",
    "BATCH_SIZE = 5000  # Number of audio files to process in each batch\n",
    "#NPZ_SHARD_SIZE = 5000  # number of vocalizations per shard before flushing to disk\n",
    "EMB_DTYPE = np.float32 # Numeric precision (float32 is good for embeddings)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "cwd = Path.cwd()\n",
    "project_root = cwd.parents[1]\n",
    "base_path = project_root / 'Output_files' / 'Extended_audios' #Path base\n",
    "output_dir = project_root / 'Output_files' / 'Embeddings_from_3sPadding' # Output root for embeddings\n",
    "output_dir.mkdir(parents=True, exist_ok=True) # Create output directory if it doesn't exist\n",
    "\n",
    "# =========================\n",
    "# Utility: find audio files\n",
    "# =========================\n",
    "def get_audio_files(base_path, folder_name, pattern=\"**/*.[Ww][Aa][Vv]\"):\n",
    "    return glob.glob(f\"{str(base_path)}/{folder_name}/{pattern}\", recursive=True)\n",
    "\n",
    "# =========================\n",
    "# Output targets per dataset\n",
    "# - For PARQUET: we save many small parquet \"parts\" (easy to append), plus a registry of processed files.\n",
    "# - For NPZ: we save shards: *_shard000.npz, *_shard001.npz, ... plus a small index of processed files.\n",
    "# =========================\n",
    "def parquet_targets_for(dataset_name: str):\n",
    "    ds_dir = output_dir / f\"{dataset_name}_parquet_parts\"\n",
    "    ds_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Registry to quickly skip already processed files\n",
    "    registry_path = ds_dir / f\"{dataset_name}_processed_files.parquet\"\n",
    "    return ds_dir, registry_path\n",
    "\n",
    "def npz_targets_for(dataset_name: str):\n",
    "    ds_dir = output_dir / f\"{dataset_name}_npz_shards\"\n",
    "    ds_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # We’ll write shards like f\"{dataset_name}_shard000.npz\"\n",
    "    shard_prefix = ds_dir / f\"{dataset_name}_shard\"\n",
    "    registry_path = ds_dir / f\"{dataset_name}_processed_files.parquet\"  # keep same registry approach\n",
    "    return ds_dir, shard_prefix, registry_path\n",
    "\n",
    "# =========================\n",
    "# Dataset map\n",
    "# Each entry: (list_of_audio_paths, output_descriptor)\n",
    "# =========================\n",
    "def out_descriptor(dataset_name: str):\n",
    "    if OUTPUT_FORMAT.lower() == \"parquet\":\n",
    "        ds_dir, registry = parquet_targets_for(dataset_name)\n",
    "        return {\"format\": \"parquet\", \"parts_dir\": ds_dir, \"registry\": registry}\n",
    "    elif OUTPUT_FORMAT.lower() == \"npz\":\n",
    "        ds_dir, shard_prefix, registry = npz_targets_for(dataset_name)\n",
    "        return {\"format\": \"npz\", \"shard_prefix\": shard_prefix, \"registry\": registry}\n",
    "    else:\n",
    "        raise ValueError(\"OUTPUT_FORMAT must be 'parquet' or 'npz'\")\n",
    "\n",
    "# Datasets\n",
    "audios_chiffchaff     = get_audio_files(base_path, \"chiffchaff-fg\")\n",
    "audios_littleowl      = get_audio_files(base_path, \"littleowl-fg\")\n",
    "audios_pipit          = get_audio_files(base_path, \"pipit-fg\")\n",
    "audios_rtbc           = get_audio_files(base_path, \"rtbc-begging\")\n",
    "audios_littlepenguin  = get_audio_files(base_path, \"littlepenguin-display_call-exhale\")\n",
    "audios_greatTit       = get_audio_files(base_path, \"greatTit_song-files\")\n",
    "audios_kiwi           = get_audio_files(base_path, \"KiwiTrimmed\")  # Kiwi dataset, if needed\n",
    "\n",
    "dataset_map = {\n",
    "    \"chiffchaff-fg\": (audios_chiffchaff,    out_descriptor(\"chiffchaff\")),\n",
    "    \"littleowl-fg\":  (audios_littleowl,     out_descriptor(\"littleowl\")),\n",
    "    \"pipit-fg\":      (audios_pipit,         out_descriptor(\"pipit\")),\n",
    "    \"rtbc-begging\":  (audios_rtbc,          out_descriptor(\"rtbc\")),\n",
    "    \"littlepenguin-display_call-exhale\": (audios_littlepenguin, out_descriptor(\"littlepenguin\")),\n",
    "    \"greatTit_song-files\": (audios_greatTit, out_descriptor(\"greatTit\")),\n",
    "    \"KiwiTrimmed\":   (audios_kiwi,          out_descriptor(\"kiwi\")),\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Dropdown for dataset selection\n",
    "# =========================\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=[(\"Select a dataset\", None)] + [(k, k) for k in dataset_map.keys()],\n",
    "    description=\"Select dataset:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"50%\"),\n",
    ")\n",
    "\n",
    "# Globals set when a dataset is picked\n",
    "audios = []\n",
    "output_desc = None\n",
    "selected_key = None \n",
    "\n",
    "def on_selection_change(change):\n",
    "    if change[\"new\"] is not None:\n",
    "        global audios, output_desc, selected_key\n",
    "        selected_key = change[\"new\"]\n",
    "        audios, output_desc = dataset_map[selected_key]\n",
    "        print(f\"\\nSelected: {selected_key}\")\n",
    "        print(f\"Total audio files: {len(audios)}\")\n",
    "        print(f\"batch size: {BATCH_SIZE} audio files per batch\")\n",
    "        if output_desc[\"format\"] == \"parquet\":\n",
    "            print(f\"Output: Parquet parts directory -> {output_desc['parts_dir'].name}\")\n",
    "            print(f\"Processed registry: {output_desc['registry'].name}\")\n",
    "        else:\n",
    "            print(f\"Output: NPZ shards prefix -> {output_desc['shard_prefix'].name}\")\n",
    "            print(f\"Processed registry: {output_desc['registry'].name}\")\n",
    "\n",
    "dataset_dropdown.observe(on_selection_change, names=\"value\")\n",
    "display(dataset_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataset: KiwiTrimmed\n",
      "Total audios in the original list: 455\n",
      "Total audios already processed (from registry): 0\n",
      "Total audios not yet processed: 455\n",
      "\n",
      "Examples of unprocessed files (up to 5):\n",
      "  • 10_2020_10_14_21_07_trim.wav\n",
      "  • 10_2020_10_15_21_57_trim.wav\n",
      "  • 11_2020_10_12_2_07_trim.wav\n",
      "  • 11_2020_10_13_21_08_trim.wav\n",
      "  • 11_2020_10_2_22_58_trim.wav\n",
      "\n",
      "Resolved 455 files to full paths.\n"
     ]
    }
   ],
   "source": [
    "# Check selection and discover processed/unprocessed files using the registry\n",
    "try:\n",
    "    assert audios is not None and output_desc is not None\n",
    "except Exception:\n",
    "    print(\"❌ Please run the dataset selection cell first.\")\n",
    "else:\n",
    "       # Build the set of original audio basenames\n",
    "    print(\"Selected dataset:\", selected_key)\n",
    "    original_audios = set(Path(audio).name for audio in audios)\n",
    "\n",
    "    # Load (or initialize) the processed-files registry (Parquet)\n",
    "    reg_path = output_desc[\"registry\"]\n",
    "    if reg_path.exists():\n",
    "        try:\n",
    "            df_reg = pd.read_parquet(reg_path)\n",
    "            if \"file_name\" in df_reg.columns:\n",
    "                processed_files = set(df_reg[\"file_name\"].astype(str).unique())\n",
    "            else:\n",
    "                processed_files = set()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not read registry at {reg_path}: {e}\")\n",
    "            processed_files = set()\n",
    "    else:\n",
    "        processed_files = set()\n",
    "\n",
    "       # Compute unprocessed files\n",
    "    unprocessed_audios = original_audios - processed_files\n",
    "\n",
    "    # Report\n",
    "    print(f\"Total audios in the original list: {len(original_audios)}\")\n",
    "    print(f\"Total audios already processed (from registry): {len(processed_files)}\")\n",
    "    print(f\"Total audios not yet processed: {len(unprocessed_audios)}\")\n",
    "\n",
    "    # (Optional) Preview a few unprocessed/processed files for sanity check\n",
    "    show_n = 5\n",
    "    if unprocessed_audios:\n",
    "        ex_unproc = sorted(list(unprocessed_audios))[:show_n]\n",
    "        print(f\"\\nExamples of unprocessed files (up to {show_n}):\")\n",
    "        for s in ex_unproc:\n",
    "            print(\"  •\", s)\n",
    "\n",
    "    if processed_files:\n",
    "        ex_proc = sorted(list(processed_files))[:show_n]\n",
    "        print(f\"\\nExamples of processed files (up to {show_n}):\")\n",
    "        for s in ex_proc:\n",
    "            print(\"  •\", s)\n",
    "\n",
    "\n",
    "    # Map from basename to full path (for this dataset)\n",
    "    path_map = {Path(p).name: p for p in audios}\n",
    "\n",
    "    # Build a list of full paths we actually need to process\n",
    "    todo_list_paths = []\n",
    "    missing_from_map = []\n",
    "    for fname in sorted(list(unprocessed_audios)):\n",
    "        if fname in path_map:\n",
    "            todo_list_paths.append(path_map[fname])\n",
    "        else:\n",
    "            missing_from_map.append(fname)\n",
    "\n",
    "    print(f\"\\nResolved {len(todo_list_paths)} files to full paths.\")\n",
    "    if missing_from_map:\n",
    "        print(f\"⚠️ {len(missing_from_map)} names were not found in path_map (skipping):\")\n",
    "        for s in missing_from_map[:5]:\n",
    "            print(\"  •\", s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels loaded.\n",
      "load model True\n",
      "Model loaded.\n",
      "Labels loaded.\n",
      "load_species_list_model\n",
      "Meta model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/455 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 455/455 [06:40<00:00,  1.14file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parquet parts written to: /teamspace/studios/this_studio/Output_files/Embeddings_from_3sPadding/kiwi_parquet_parts\n"
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = None  # fallback if tqdm isn't available\n",
    "\n",
    "# =========================\n",
    "# Embedding extraction + saving (Parquet or NPZ)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "import os, re, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Safety checks\n",
    "if \"audios\" not in globals() or \"output_desc\" not in globals():\n",
    "    raise RuntimeError(\"❌ Please run the dataset selection cell first.\")\n",
    "if \"unprocessed_audios\" not in globals():\n",
    "    raise RuntimeError(\"❌ Please run the registry cell to compute 'unprocessed_audios'.\")\n",
    "\n",
    "if len(unprocessed_audios) == 0:\n",
    "    print(\"✅ All audio files have already been processed. No new files to process.\")\n",
    "else:\n",
    "    # Initialize BirdNET once\n",
    "    analyzer = Analyzer()\n",
    "\n",
    "    # Helpers to derive next file index (part/shard)\n",
    "    def next_parquet_part_idx(parts_dir: Path) -> int:\n",
    "        pattern = re.compile(r\"part_(\\d{4})\\.parquet$\")\n",
    "        idxs = []\n",
    "        for p in parts_dir.glob(\"part_*.parquet\"):\n",
    "            m = pattern.search(p.name)\n",
    "            if m: idxs.append(int(m.group(1)))\n",
    "        return (max(idxs) + 1) if idxs else 0\n",
    "\n",
    "    def next_npz_shard_idx(prefix: Path) -> int:\n",
    "        # matches e.g. mydataset_shard012.npz\n",
    "        pattern = re.compile(re.escape(prefix.name) + r\"(\\d{3})\\.npz$\")\n",
    "        idxs = []\n",
    "        for p in prefix.parent.glob(prefix.name + \"*.npz\"):\n",
    "            m = pattern.match(p.name)\n",
    "            if m: idxs.append(int(m.group(1)))\n",
    "        return (max(idxs) + 1) if idxs else 0\n",
    "\n",
    "    # Load current registry (so we can append to it after each flush)\n",
    "    reg_path = output_desc[\"registry\"]\n",
    "    if reg_path.exists():\n",
    "        df_reg = pd.read_parquet(reg_path)\n",
    "        processed_files = set(df_reg[\"file_name\"].astype(str).unique()) if \"file_name\" in df_reg.columns else set()\n",
    "    else:\n",
    "        df_reg = pd.DataFrame(columns=[\"file_name\"])\n",
    "        processed_files = set()\n",
    "\n",
    "    # Iterate only over unprocessed files\n",
    "    # todo_list = sorted(list(unprocessed_audios))\n",
    "    if \"todo_list_paths\" not in globals() or not todo_list_paths:\n",
    "        raise RuntimeError(\"❌ 'todo_list_paths' is missing. Re-run the registry cell.\")\n",
    "\n",
    "    # Common controls\n",
    "    #batch_size = 1000  # how often to flush (for Parquet)\n",
    "    processed_in_this_run = []\n",
    "\n",
    "    if output_desc[\"format\"] == \"parquet\":\n",
    "        parts_dir: Path = output_desc[\"parts_dir\"]\n",
    "        part_idx = next_parquet_part_idx(parts_dir)\n",
    "\n",
    "        rows = []  # will accumulate dict rows for a Parquet part\n",
    "\n",
    "        #for k, audio_path in enumerate(todo_list_paths, 1):\n",
    "        for k, audio_path in enumerate( tqdm(todo_list_paths, desc=\"Extracting embeddings\", unit=\"file\"), 1):\n",
    "            try:\n",
    "                file_name = os.path.basename(audio_path)\n",
    "\n",
    "                # BirdNET extraction\n",
    "                #recording = Recording(analyzer, audio_path)\n",
    "                #recording.extract_embeddings()\n",
    "\n",
    "                # BirdNET extraction (silence internal prints)\n",
    "                with open(os.devnull, \"w\") as fnull, redirect_stdout(fnull), redirect_stderr(fnull):\n",
    "                    recording = Recording(analyzer, audio_path)\n",
    "                    recording.extract_embeddings()\n",
    "\n",
    "                # Build rows (one row per frame)\n",
    "                for emb in recording.embeddings:\n",
    "                    row = {\n",
    "                        \"file_name\": file_name,\n",
    "                        \"start_time\": float(emb[\"start_time\"]),\n",
    "                        \"end_time\": float(emb[\"end_time\"]),\n",
    "                    }\n",
    "                    # emb[\"embeddings\"] is a 1024-dim vector\n",
    "                    # store as float32 for compactness\n",
    "                    for i, v in enumerate(emb[\"embeddings\"]):\n",
    "                        row[f\"dim_{i+1}\"] = np.float32(v)\n",
    "                    rows.append(row)\n",
    "\n",
    "                processed_in_this_run.append(file_name)\n",
    "\n",
    "                # Flush every batch_size files\n",
    "                if (k % BATCH_SIZE == 0) and rows:\n",
    "                    df_chunk = pd.DataFrame(rows)\n",
    "                    float_cols = [c for c in df_chunk.columns if c.startswith(\"dim_\")]\n",
    "                    if float_cols:\n",
    "                        df_chunk[float_cols] = df_chunk[float_cols].astype(\"float32\")\n",
    "\n",
    "                    out_part = parts_dir / f\"part_{part_idx:04d}.parquet\"\n",
    "                    df_chunk.to_parquet(out_part, index=False, engine=PARQUET_ENGINE, compression=PARQUET_COMPRESSION)\n",
    "                    rows.clear()\n",
    "                    part_idx += 1\n",
    "\n",
    "                    # Update registry\n",
    "                    df_new = pd.DataFrame({\"file_name\": processed_in_this_run})\n",
    "                    df_reg = pd.concat([df_reg, df_new], ignore_index=True).drop_duplicates(subset=[\"file_name\"])\n",
    "                    df_reg.to_parquet(reg_path, index=False, engine=PARQUET_ENGINE, compression=PARQUET_COMPRESSION)\n",
    "                    processed_in_this_run.clear()\n",
    "\n",
    "                del recording\n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "        # Final flush\n",
    "        if rows:\n",
    "            df_chunk = pd.DataFrame(rows)\n",
    "            float_cols = [c for c in df_chunk.columns if c.startswith(\"dim_\")]\n",
    "            if float_cols:\n",
    "                df_chunk[float_cols] = df_chunk[float_cols].astype(\"float32\")\n",
    "\n",
    "            out_part = parts_dir / f\"part_{part_idx:04d}.parquet\"\n",
    "            df_chunk.to_parquet(out_part, index=False, engine=PARQUET_ENGINE, compression=PARQUET_COMPRESSION)\n",
    "            rows.clear()\n",
    "            part_idx += 1\n",
    "\n",
    "        # Update registry for any remaining processed files\n",
    "        if processed_in_this_run:\n",
    "            df_new = pd.DataFrame({\"file_name\": processed_in_this_run})\n",
    "            df_reg = pd.concat([df_reg, df_new], ignore_index=True).drop_duplicates(subset=[\"file_name\"])\n",
    "            df_reg.to_parquet(reg_path, index=False, engine=PARQUET_ENGINE, compression=PARQUET_COMPRESSION)\n",
    "\n",
    "        print(f\"✅ Parquet parts written to: {parts_dir}\")\n",
    "\n",
    "    # elif output_desc[\"format\"] == \"npz\":\n",
    "    #     shard_prefix: Path = output_desc[\"shard_prefix\"]\n",
    "    #     shard_idx = next_npz_shard_idx(shard_prefix)\n",
    "\n",
    "    #     # Buffers for the current shard\n",
    "    #     emb_list = []    # list of (T_i, 1024) arrays\n",
    "    #     lengths = []     # list of T_i\n",
    "    #     fnames = []      # one per vocalization\n",
    "    #     starts = []      # optional: first start_time\n",
    "    #     ends = []        # optional: last end_time\n",
    "\n",
    "    #     def flush_npz_shard():\n",
    "    #         nonlocal shard_idx, emb_list, lengths, fnames, starts, ends, df_reg\n",
    "\n",
    "    #         if not lengths:\n",
    "    #             return\n",
    "\n",
    "    #         lengths_arr = np.asarray(lengths, dtype=np.int32)\n",
    "    #         offsets = np.zeros(len(lengths_arr) + 1, dtype=np.int64)\n",
    "    #         offsets[1:] = np.cumsum(lengths_arr)\n",
    "\n",
    "    #         D = 1024\n",
    "    #         X = np.empty((int(offsets[-1]), D), dtype=EMB_DTYPE)\n",
    "    #         pos = 0\n",
    "    #         for E in emb_list:\n",
    "    #             n = E.shape[0]\n",
    "    #             X[pos:pos+n] = E\n",
    "    #             pos += n\n",
    "\n",
    "    #         out_npz = shard_prefix.parent / f\"{shard_prefix.name}{shard_idx:03d}.npz\"\n",
    "    #         np.savez_compressed(\n",
    "    #             out_npz,\n",
    "    #             X=X, lengths=lengths_arr, offsets=offsets,\n",
    "    #             starts=np.asarray(starts, dtype=np.float32),\n",
    "    #             ends=np.asarray(ends, dtype=np.float32),\n",
    "    #         )\n",
    "    #         # filenames as separate .npy\n",
    "    #         out_names = shard_prefix.parent / f\"{shard_prefix.name}{shard_idx:03d}_filenames.npy\"\n",
    "    #         np.save(out_names, np.array(fnames, dtype=object))\n",
    "\n",
    "    #         # Update registry\n",
    "    #         df_new = pd.DataFrame({\"file_name\": fnames})\n",
    "    #         df_reg = pd.concat([df_reg, df_new], ignore_index=True).drop_duplicates(subset=[\"file_name\"])\n",
    "    #         df_reg.to_parquet(reg_path, index=False, engine=PARQUET_ENGINE, compression=PARQUET_COMPRESSION)\n",
    "\n",
    "    #         # Reset buffers\n",
    "    #         emb_list.clear(); lengths.clear(); fnames.clear(); starts.clear(); ends.clear()\n",
    "    #         shard_idx += 1\n",
    "\n",
    "    #     for k, audio_path in enumerate(todo_list, 1):\n",
    "    #         try:\n",
    "    #             file_name = os.path.basename(audio_path)\n",
    "\n",
    "    #             recording = Recording(analyzer, audio_path)\n",
    "    #             recording.extract_embeddings()\n",
    "\n",
    "    #             # Build (T, 1024) matrix and start/end\n",
    "    #             E = np.vstack([np.asarray(emb[\"embeddings\"], dtype=EMB_DTYPE)\n",
    "    #                            for emb in recording.embeddings])  # (T, 1024)\n",
    "    #             s0 = float(recording.embeddings[0][\"start_time\"])\n",
    "    #             eN = float(recording.embeddings[-1][\"end_time\"])\n",
    "\n",
    "    #             emb_list.append(E)\n",
    "    #             lengths.append(E.shape[0])\n",
    "    #             fnames.append(file_name)\n",
    "    #             starts.append(s0)\n",
    "    #             ends.append(eN)\n",
    "\n",
    "    #             # Flush shard if needed\n",
    "    #             if len(fnames) >= NPZ_SHARD_SIZE:\n",
    "    #                 flush_npz_shard()\n",
    "\n",
    "    #             del recording\n",
    "    #             gc.collect()\n",
    "\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "    #     # Final shard\n",
    "    #     flush_npz_shard()\n",
    "    #     print(f\"✅ NPZ shards written with prefix: {shard_prefix.name}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"OUTPUT_FORMAT must be 'parquet' or 'npz'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audios processed: 455\n",
      "Total audios not processed: 0\n",
      "Unprocessed audios:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the Parquet registry exists to retrieve processed audio files\n",
    "registry_path = output_desc[\"registry\"]\n",
    "\n",
    "if registry_path.exists():\n",
    "    df_embeddings = pd.read_parquet(registry_path, engine=PARQUET_ENGINE)\n",
    "    processed_files = set(df_embeddings[\"file_name\"])  # Already processed files\n",
    "else:\n",
    "    df_embeddings = pd.DataFrame()\n",
    "    processed_files = set()\n",
    "\n",
    "# Original audios (only filenames, not full paths)\n",
    "original_audios = set(Path(audio).name for audio in audios)\n",
    "\n",
    "# Already processed audios\n",
    "processed_audios = set(df_embeddings[\"file_name\"].unique()) if not df_embeddings.empty else set()\n",
    "\n",
    "# Unprocessed audios\n",
    "unprocessed_audios = original_audios - processed_audios\n",
    "\n",
    "# Display results\n",
    "print(f\"Total audios processed: {len(processed_audios)}\")\n",
    "print(f\"Total audios not processed: {len(unprocessed_audios)}\")\n",
    "\n",
    "# Audios that were not processed\n",
    "print(\"Unprocessed audios:\")\n",
    "print(\"\\n\".join(unprocessed_audios))\n",
    "\n",
    "# Clean up memory\n",
    "del df_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parquet files in folder:\n",
      "  • kiwi_processed_files.parquet  (REGISTRY?)\n",
      "  • part_0000.parquet  (PART)\n",
      "NaN rate across embedding dims: 0.000000\n",
      "✅ Loaded 4421 rows from 1 part files.\n",
      "📂 Dataset: KiwiTrimmed\n",
      "🎵 Total audio files in folder: 455\n",
      "                      file_name  start_time  end_time     dim_1  dim_2  \\\n",
      "0  10_2020_10_14_21_07_trim.wav         0.0       3.0  0.339824    0.0   \n",
      "1  10_2020_10_14_21_07_trim.wav         3.0       6.0  1.204902    0.0   \n",
      "2  10_2020_10_14_21_07_trim.wav         6.0       9.0  0.822589    0.0   \n",
      "3  10_2020_10_14_21_07_trim.wav         9.0      12.0  0.482953    0.0   \n",
      "4  10_2020_10_14_21_07_trim.wav        12.0      15.0  0.332097    0.0   \n",
      "\n",
      "      dim_3     dim_4     dim_5  dim_6     dim_7  ...  dim_1015  dim_1016  \\\n",
      "0  1.098615  0.031068  1.136220    0.0  0.637314  ...       0.0  0.467335   \n",
      "1  0.514429  0.081455  1.107298    0.0  0.393557  ...       0.0  0.259372   \n",
      "2  0.137139  0.270432  0.878936    0.0  0.529867  ...       0.0  0.017661   \n",
      "3  0.014902  0.693020  0.570054    0.0  0.540671  ...       0.0  0.000000   \n",
      "4  0.009046  1.043191  0.339056    0.0  0.754979  ...       0.0  0.017689   \n",
      "\n",
      "   dim_1017  dim_1018  dim_1019  dim_1020  dim_1021  dim_1022  dim_1023  \\\n",
      "0  0.520109  0.988759  0.047462  0.295802  0.939173  0.004887  0.904417   \n",
      "1  0.544719  0.576579  0.348806  0.447666  0.757956  0.727544  0.247254   \n",
      "2  0.384980  0.132754  0.083928  1.206224  1.494729  0.489981  0.273870   \n",
      "3  0.092070  0.062009  0.015871  0.547037  2.253998  0.345286  0.488413   \n",
      "4  0.158960  0.096461  0.105675  0.727457  2.145851  0.346037  0.223563   \n",
      "\n",
      "   dim_1024  \n",
      "0  0.589950  \n",
      "1  0.428228  \n",
      "2  0.302167  \n",
      "3  0.038171  \n",
      "4  0.085689  \n",
      "\n",
      "[5 rows x 1027 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_selected_embeddings(debug: bool = False):\n",
    "    \"\"\"\n",
    "    Load embeddings for the dataset currently selected via the dropdown.\n",
    "    Uses globals: selected_dataset_key, audios, output_desc.\n",
    "    Only reads real part files (excludes registry).\n",
    "    \"\"\"\n",
    "    # Safety checks\n",
    "    if 'output_desc' not in globals() or output_desc is None:\n",
    "        print(\"❌ No dataset selected yet. Pick one in the dropdown first.\")\n",
    "        return None\n",
    "    if 'selected_key' not in globals() or not selected_key:\n",
    "        print(\"❌ No dataset selected yet. Pick one in the dropdown first.\")\n",
    "        return None\n",
    "\n",
    "    if output_desc[\"format\"] != \"parquet\":\n",
    "        print(\"⚠ Current dataset is not using Parquet.\")\n",
    "        return None\n",
    "\n",
    "    parts_dir: Path = output_desc[\"parts_dir\"]\n",
    "    if not parts_dir.exists():\n",
    "        print(f\"❌ Parts directory does not exist: {parts_dir}\")\n",
    "        return None\n",
    "\n",
    "    # ✅ Only the embedding parts, not the registry\n",
    "    part_files = sorted(parts_dir.glob(\"part_*.parquet\"))\n",
    "    if debug:\n",
    "        print(\"All parquet files in folder:\")\n",
    "        for p in sorted(parts_dir.glob(\"*.parquet\")):\n",
    "            tag = \" (PART)\" if p.name.startswith(\"part_\") else \" (REGISTRY?)\"\n",
    "            print(\"  •\", p.name, tag)\n",
    "\n",
    "    if not part_files:\n",
    "        print(f\"⚠ No part_*.parquet found in {parts_dir}\")\n",
    "        return None\n",
    "\n",
    "    # Read & concat\n",
    "    df = pd.concat([pd.read_parquet(f, engine=PARQUET_ENGINE) for f in part_files],\n",
    "                   ignore_index=True)\n",
    "\n",
    "    # Optional sanity check\n",
    "    dim_cols = [c for c in df.columns if c.startswith(\"dim_\")]\n",
    "    if dim_cols:\n",
    "        nan_rate = float(df[dim_cols].isna().mean().mean())\n",
    "        if debug:\n",
    "            print(f\"NaN rate across embedding dims: {nan_rate:.6f}\")\n",
    "\n",
    "    print(f\"✅ Loaded {len(df)} rows from {len(part_files)} part files.\")\n",
    "    print(f\"📂 Dataset: {selected_key}\")\n",
    "    print(f\"🎵 Total audio files in folder: {len(audios)}\")\n",
    "    return df\n",
    "\n",
    "df = load_selected_embeddings(debug=True)\n",
    "if df is not None:\n",
    "    print(df.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
